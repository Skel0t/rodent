enum MemoryFormat {
    CHW, HWC
}

struct Matrix {
    buf      : Buffer,
    offset   : i64,
    channels : i32,
    format   : MemoryFormat,
    rows     : i32,
    cols     : i32
}

struct AccM {
    read  : fn(i32, i32, i32) -> f32,
    write : fn(i32, i32, i32, f32) -> ()
}

fn @make_matrix(buffer: Buffer, offset: i64, format: MemoryFormat, channels: i32, rows: i32, cols: i32) -> Matrix {
    Matrix { buf = buffer, offset = offset, format = format, channels = channels, rows = rows, cols = cols }
}

/* Returns a Matrix accessor, no matter the memory format (CHW or HWC)
   Expects row-major matrix storage layout */
fn @get_mat_acc(m: Matrix) -> AccM {
    fn @get_mat_acc_hwc(m: Matrix) -> AccM {
        AccM {
            read  = @|row, col, chn|      { bitcast[&   [f32]](m.buf.data)(m.offset + ((row * m.cols + col) * m.channels + chn) as i64) },
            write = @|row, col, chn, val| { bitcast[&mut[f32]](m.buf.data)(m.offset + ((row * m.cols + col) * m.channels + chn) as i64) = val; }
        }
    }

    fn @get_mat_acc_chw(m: Matrix) -> AccM {
        AccM {
            read  = @|row, col, chn|      { bitcast[&   [f32]](m.buf.data)(m.offset + (row * m.cols + col + chn * m.cols * m.rows) as i64) },
            write = @|row, col, chn, val| { bitcast[&mut[f32]](m.buf.data)(m.offset + (row * m.cols + col + chn * m.cols * m.rows) as i64) = val; }
        }
    }

    match m.format {
        MemoryFormat::CHW => get_mat_acc_chw(m),
        MemoryFormat::HWC => get_mat_acc_hwc(m)
    }
}

/* Takes two matrix accessors and the number of channels the first one is accessing.
   Returns a concatenated accessor, accessing the channels of the second accessor
   if the given channel exceeds channel count of the first accessor. */
fn @get_cat_mat_acc(acc1: AccM, acc2: AccM, channels1: i32) -> AccM {
    AccM {
        read  = @|row, col, chn|      { if chn < channels1 { acc1.read(row, col, chn)        } else { acc2.read(row, col, chn - channels1) } },
        write = @|row, col, chn, val| { if chn < channels1 { acc1.write(row, col, chn, val); } else { acc2.write(row, col, chn - channels1, val); } }
    }
}

/* a and b need to have same channel format. */
fn @matmul_cpu(a: Matrix, b: Matrix, biases: fn(i32) -> f32, act_fn: fn(f32) -> f32, buf: Buffer, off: i64) -> Matrix {
    let mat = make_matrix(buf, off, MemoryFormat::HWC, 1, a.rows, b.cols);
    let a_acc = get_mat_acc(a);
    let b_acc = get_mat_acc(b);
    let m_acc = get_mat_acc(mat);

    /* Constant variables defining the block size */
    let c_size = 64;
    let i_size = 16;

    let c_vec_len = round_down(mat.cols, c_size);
    let i_vec_len = round_down(a.cols, i_size);

    /* Vectorize over mat.cols with c_size vectors */
    for c in parallel_step(0, c_vec_len, c_size) {
        /* Needs c_size many elements. Even though it is known at compile-time
           AnyDSL doesn't currently support putting c_size here */
        let mut m : [f32 * 64];

        /* ii loop for ii = 0 */
        for r in range(0, mat.rows) {
            let b = biases(r);
            vectorize(64, @|j| {
                m(j) = b;
                for i in unroll(0, i_size) {
                    m(j) += a_acc.read(r, i, 0) * b_acc.read(i, c + j, 0);
                }
                m_acc.write(r, c + j, 0, m(j));
            });
        }

        /* ii loop for ii > 0 and last multiple of i_size < ii */
        for ii in range_step(i_size, i_vec_len, i_size) {
            for r in range(0, mat.rows) {
                vectorize(64, @|j| {
                    m(j) = m_acc.read(r, c + j, 0);
                    for i_loop in unroll(0, i_size) {
                        let i = i_loop + ii;
                        m(j) += a_acc.read(r, i, 0) * b_acc.read(i, c + j, 0);
                    }
                    m_acc.write(r, c + j, 0, m(j));
                });
            }
        }

        /* ii loop for ii > (last multiple of i_size < ii) */
        for r in range(0, mat.rows) {
            for c_inner in range(c, imin(c + c_size, mat.cols)) {
                m(0) = m_acc.read(r, c_inner, 0);
                for i in range(i_vec_len, a.cols) {
                    m(0) += a_acc.read(r, i, 0) * b_acc.read(i, c_inner, 0);
                }
                m_acc.write(r, c_inner, 0, act_fn(m(0)));
            }
        }
    }

    /* Do the remaining part without vectorization */
    for c in parallel(0, c_vec_len, mat.cols) {
        let mut m : [f32 * 64];

        /* ii loop for ii = 0 */
        for r in range(0, mat.rows) {
            m(0) = biases(r);
            for i_loop in unroll(0, i_size) {
                let i = i_loop;
                m(0) += a_acc.read(r, i, 0) * b_acc.read(i, c, 0);
            }
            m_acc.write(r, c, 0, m(0));
        }

        /* ii loop for ii > 0 and last multiple of i_size < ii */
        for ii in range_step(i_size, i_vec_len, i_size) {
            for r in range(0, mat.rows) {
                m(0) = m_acc.read(r, c, 0);
                for i_loop in unroll(0, i_size) {
                    let i = i_loop + ii;
                    m(0) += a_acc.read(r, i, 0) * b_acc.read(i, c, 0);
                }
                m_acc.write(r, c, 0, m(0));
            }
        }

        /* ii loop for ii > (last multiple of i_size < ii) */
        for r in range(0, mat.rows) {
            m(0) = m_acc.read(r, c, 0);
            for i in range(i_vec_len, a.cols) {
                m(0) += a_acc.read(r, i, 0) * b_acc.read(i, c, 0);
            }
            m_acc.write(r, c, 0, act_fn(m(0)));
        }
    }

    mat
}

/* a and b need to have same channel format */
fn @matmul_oneapi(a: Matrix, b: Matrix, biases: fn(i32) -> f32, act_fn: fn(f32) -> f32, buf: Buffer, off: i64) {
    let c = make_matrix(buf, off, MemoryFormat::HWC, 1, a.rows, b.cols);
    mkl_blas_mm_mult(/*m*/ a.rows, /*n*/ b.cols, /*k*/ a.cols,
        /*a*/ bitcast[&   [f32]](&(a.buf.data(4 * a.offset))), /*lda*/ a.cols,
        /*b*/ bitcast[&   [f32]](&(b.buf.data(4 * b.offset))), /*ldb*/ b.cols,
        /*c*/ bitcast[&mut[f32]](&(c.buf.data(4 * c.offset))), /*ldc*/ c.cols);

    for acc, v, row, col, chn in iterate_matrix_par(c) {
        acc.write(row, col, chn, act_fn(v + biases(row)));  // 1 channel per row, thus access  row-th bias
    }
    c
}

fn @matmul_gpu(a: Matrix, b: Matrix, biases: fn(i32) -> f32, act_fn: fn(f32) -> f32, buf: Buffer, off: i64) {
    let acc = cuda_accelerator(0);

    let c = make_matrix(buf, off, MemoryFormat::HWC, 1, a.rows, b.cols);

    let a_acc = get_mat_acc(a);
    let b_acc = get_mat_acc(b);
    let c_acc = get_mat_acc(c);

    let threads_x = 32;
    let threads_y = 32;
    let block = (threads_x, threads_y, 1);    // 32 * 32 * 1 = 1024 Threads per block
    let grid  = (round_up(c.cols, threads_x), round_up(c.rows, threads_y), 1);

    for work_item in acc.exec(grid, block) {
        let row = work_item.bidy() * work_item.bdimy() + work_item.tidy();
        let col = work_item.bidx() * work_item.bdimx() + work_item.tidx();

        if row < c.rows && col < c.cols {
            let mut v = biases(row);
            for i in range(0, a.cols) {
                v = v + a_acc.read(row, i, 0) * b_acc.read(i, col, 0);
            }
            c_acc.write(row, col, 0, act_fn(v));
        }
    }
    acc.sync();

    c
}

// Two cublas implementations to check if there is a difference.
fn @matmul_cublas(a: Matrix, b: Matrix, biases: fn(i32) -> f32, act_fn: fn(f32) -> f32, buf: Buffer, off: i64) {
    let device = 0;
    let acc = cuda_accelerator(device);

    let c = make_matrix(buf, off, MemoryFormat::HWC, 1, a.rows, b.cols);
    let c_acc = get_mat_acc(c);

    let threads_x = 32;
    let threads_y = 32;
    let block = (threads_x, threads_y, 1);
    let grid  = (round_up(c.cols, threads_x), round_up(c.rows, threads_y), c.channels);

    cublas_S_gemm(
        /*a*/ bitcast[&   [f32]](&(a.buf.data(4 * a.offset))),
        /*b*/ bitcast[&   [f32]](&(b.buf.data(4 * b.offset))),
        /*c*/ bitcast[&mut[f32]](&(c.buf.data(4 * c.offset))),
        /*lda*/ a.cols, /*ldb*/ a.rows, /*ldc*/ b.cols, device);

    for work_item in acc.exec(grid, block) {
        let col = work_item.bdimx() * work_item.bidx() + work_item.tidx();
        let row = work_item.bdimy() * work_item.bidy() + work_item.tidy();
        let chn = work_item.bidz();     // Will never overshoot, always valid

        if row < c.rows && col < c.cols {
            // Valid thread, do same calculation as in cpu here
            c_acc.write(row, col, chn, act_fn(c_acc.read(row, col, chn) + biases(row)));  // 1 channel per row, thus access row-th bias
        }
    }
    acc.sync();

    c
}

fn @matmul_cublaslt(a: Matrix, b: Matrix, biases: fn(i32) -> f32, act_fn: fn(f32) -> f32, buf: Buffer, off: i64) {
    let device = 0;
    let acc = cuda_accelerator(device);

    let c = make_matrix(buf, off, MemoryFormat::HWC, 1, a.rows, b.cols);
    let c_acc = get_mat_acc(c);

    let threads_x = 32;
    let threads_y = 32;
    let block = (threads_x, threads_y, 1);
    let grid  = (round_up(c.cols, threads_x), round_up(c.rows, threads_y), c.channels);

    // cublas kernels do not wait for kernels computing a and b to finish
    acc.sync();

    cublaslt_S_gemm(
        /*a*/ bitcast[&   [f32]](&(a.buf.data(4 * a.offset))),
        /*b*/ bitcast[&   [f32]](&(b.buf.data(4 * b.offset))),
        /*c*/ bitcast[&mut[f32]](&(c.buf.data(4 * c.offset))),
        /*lda*/ a.cols, /*ldb*/ a.rows, /*ldc*/ b.cols, device);

    for work_item in acc.exec(grid, block) {
        let col = work_item.bdimx() * work_item.bidx() + work_item.tidx();
        let row = work_item.bdimy() * work_item.bidy() + work_item.tidy();
        let chn = work_item.bidz();     // Will never overshoot, always valid

        if row < c.rows && col < c.cols {
            // Valid thread, do same calculation as in cpu here
            c_acc.write(row, col, chn, act_fn(c_acc.read(row, col, chn) + biases(row)));  // 1 channel per row, thus access row-th bias
        }
    }
    acc.sync();

    c
}

/* Matrices a and b need to have the same dimensions. */
fn @add_element_wise_cpu(a: Matrix, b_acc: AccM, buf: Buffer, off: i64) -> Matrix {
    let res = make_matrix(buf, off, a.format, a.channels, a.rows, a.cols);

    let res_acc  = get_mat_acc(res);

    for _acc, v, r, c, chn in iterate_matrix_par(a) {
        res_acc.write(r, c, chn, b_acc.read(r, c, chn) + v);
    }

    res
}

/* Matrices a and b need to have the same dimensions. */
fn @add_element_wise_gpu(a: Matrix, b_acc: AccM, buf: Buffer, off: i64) -> Matrix {
    let acc = cuda_accelerator(0);

    let res_mat = make_matrix(buf, off, a.format, a.channels, a.rows, a.cols);

    let res_acc = get_mat_acc(res_mat);
    let a_acc   = get_mat_acc(a);

    let threads_x = 32;
    let threads_y = 32;
    let block = (threads_x, threads_y, 1);
    let grid  = (round_up(res_mat.cols, threads_x), round_up(res_mat.rows, threads_y), res_mat.channels);

    for work_item in acc.exec(grid, block) {
        let col = work_item.bdimx() * work_item.bidx() + work_item.tidx();
        let row = work_item.bdimy() * work_item.bidy() + work_item.tidy();
        let chn = work_item.bidz();     // Will never overshoot, always valid

        if row < res_mat.rows && col < res_mat.cols {
            res_acc.write(row, col, chn, b_acc.read(row, col, chn) + a_acc.read(row, col, chn));
        }
    }
    acc.sync();

    res_mat
}

/* Copies the given matrix to the given buffer and offset in HWC format. */
fn @chw_to_hwc_cpu(img_mat: Matrix, buf: Buffer, off: i64) -> Matrix {
    let res = make_matrix(buf, off, MemoryFormat::HWC, img_mat.channels, img_mat.rows, img_mat.cols);

    let img_acc = get_mat_acc(img_mat);

    for res_acc, _v, row, col, chn in iterate_matrix_par(res) {
        res_acc.write(row, col, chn, img_acc.read(row, col, chn));
    }

    res
}

fn @chw_to_hwc_gpu(img_mat: Matrix, buf: Buffer, off: i64) -> Matrix {
    let acc = cuda_accelerator(0);

    let res_mat = make_matrix(buf, off, MemoryFormat::HWC, img_mat.channels, img_mat.rows, img_mat.cols);
    let img_acc = get_mat_acc(img_mat);
    let res_acc = get_mat_acc(res_mat);

    let threads_x = 32;
    let threads_y = 32;
    let block = (threads_x, threads_y, 1);
    let grid  = (round_up(res_mat.cols, threads_x), round_up(res_mat.rows, threads_y), res_mat.channels);

    for work_item in acc.exec(grid, block) {
        let col = work_item.bdimx() * work_item.bidx() + work_item.tidx();
        let row = work_item.bdimy() * work_item.bidy() + work_item.tidy();
        let chn = work_item.bidz();     // Will never overshoot, always valid

        if row < res_mat.rows && col < res_mat.cols {
            res_acc.write(row, col, chn, img_acc.read(row, col, chn));
        }
    }
    acc.sync();

    res_mat
}
